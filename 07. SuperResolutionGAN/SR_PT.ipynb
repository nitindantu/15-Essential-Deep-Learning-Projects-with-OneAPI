{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XZFrKWHwEf7h"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchsummary\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
      "Installing collected packages: torchsummary\n",
      "Successfully installed torchsummary-1.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DsUYd7GhEf7m"
   },
   "source": [
    "Importing the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RC5YKYGuEf7n"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as n\n",
    "import torch.nn.functional as f\n",
    "import numpy as np\n",
    "import os\n",
    "from torchsummary import summary\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torchvision import models\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2VN1wknpEf7q"
   },
   "source": [
    "Defining the celebrity dataset folder and listing all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4HEAwzAFEf7r"
   },
   "outputs": [],
   "source": [
    "celebData = \"celeba-dataset/img_align_celeba/img_align_celeba/\"\n",
    "images = os.listdir(celebData)\n",
    "imageList = images[:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dOAZSX60Ef7t",
    "outputId": "75856944-bc8f-4eea-d22c-1d3744657c11"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(imageList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5pvl-X2BEf7w"
   },
   "source": [
    "Assigning the device as cuda if GPU is available else cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3l4EmLQgEf7w"
   },
   "outputs": [],
   "source": [
    "cuda = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wtQvgarfEf7z"
   },
   "source": [
    "Defining the generator architecture as given the reference paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yWoZ2TtvEf7z"
   },
   "outputs": [],
   "source": [
    "class Generator(n.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = n.Conv2d(3,64,9,padding=4,bias=False)\n",
    "        self.conv2 = n.Conv2d(64,64,3,padding=1,bias=False)\n",
    "        self.conv3_1 = n.Conv2d(64,256,3,padding=1,bias=False)\n",
    "        self.conv3_2 = n.Conv2d(64,256,3,padding=1,bias=False)\n",
    "        self.conv4 = n.Conv2d(64,3,9,padding=4,bias=False)\n",
    "        self.bn = n.BatchNorm2d(64)\n",
    "        self.ps = n.PixelShuffle(2)\n",
    "        self.prelu = n.PReLU()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        block1 = self.prelu(self.conv1(x))\n",
    "        block2 = torch.add(self.bn(self.conv2(self.prelu(self.bn(self.conv2(block1))))),block1)\n",
    "        block3 = torch.add(self.bn(self.conv2(self.prelu(self.bn(self.conv2(block2))))),block2)\n",
    "        block4 = torch.add(self.bn(self.conv2(self.prelu(self.bn(self.conv2(block3))))),block3)\n",
    "        block5 = torch.add(self.bn(self.conv2(self.prelu(self.bn(self.conv2(block4))))),block4)\n",
    "        block6 = torch.add(self.bn(self.conv2(self.prelu(self.bn(self.conv2(block5))))),block5)\n",
    "        block7 = torch.add(self.bn(self.conv2(block6)),block1)\n",
    "        block8 = self.prelu(self.ps(self.conv3_1(block7)))\n",
    "        block9 = self.prelu(self.ps(self.conv3_2(block8)))\n",
    "        block10 = self.conv4(block9)\n",
    "        return block10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yVcX-zDaEf71"
   },
   "source": [
    "Assigning the Generator to cuda(if available) and printing the summary of the Generator with a dummy input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kBHl0rO_Ef72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 64, 64]          15,552\n",
      "             PReLU-2           [-1, 64, 64, 64]               1\n",
      "            Conv2d-3           [-1, 64, 64, 64]          36,864\n",
      "       BatchNorm2d-4           [-1, 64, 64, 64]             128\n",
      "             PReLU-5           [-1, 64, 64, 64]               1\n",
      "            Conv2d-6           [-1, 64, 64, 64]          36,864\n",
      "       BatchNorm2d-7           [-1, 64, 64, 64]             128\n",
      "            Conv2d-8           [-1, 64, 64, 64]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 64, 64]             128\n",
      "            PReLU-10           [-1, 64, 64, 64]               1\n",
      "           Conv2d-11           [-1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-12           [-1, 64, 64, 64]             128\n",
      "           Conv2d-13           [-1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-14           [-1, 64, 64, 64]             128\n",
      "            PReLU-15           [-1, 64, 64, 64]               1\n",
      "           Conv2d-16           [-1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-17           [-1, 64, 64, 64]             128\n",
      "           Conv2d-18           [-1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-19           [-1, 64, 64, 64]             128\n",
      "            PReLU-20           [-1, 64, 64, 64]               1\n",
      "           Conv2d-21           [-1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-22           [-1, 64, 64, 64]             128\n",
      "           Conv2d-23           [-1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-24           [-1, 64, 64, 64]             128\n",
      "            PReLU-25           [-1, 64, 64, 64]               1\n",
      "           Conv2d-26           [-1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-27           [-1, 64, 64, 64]             128\n",
      "           Conv2d-28           [-1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-29           [-1, 64, 64, 64]             128\n",
      "           Conv2d-30          [-1, 256, 64, 64]         147,456\n",
      "     PixelShuffle-31         [-1, 64, 128, 128]               0\n",
      "            PReLU-32         [-1, 64, 128, 128]               1\n",
      "           Conv2d-33        [-1, 256, 128, 128]         147,456\n",
      "     PixelShuffle-34         [-1, 64, 256, 256]               0\n",
      "            PReLU-35         [-1, 64, 256, 256]               1\n",
      "           Conv2d-36          [-1, 3, 256, 256]          15,552\n",
      "================================================================\n",
      "Total params: 732,936\n",
      "Trainable params: 732,936\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 179.50\n",
      "Params size (MB): 2.80\n",
      "Estimated Total Size (MB): 182.34\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "gen = Generator().to(cuda)\n",
    "\n",
    "#Uncomment below mentioned three lines if you have more than one gpu and want to use all of them\n",
    "#ngpu=2\n",
    "# if (cuda.type == 'cuda') and (ngpu > 1):\n",
    "#     gen = n.DataParallel(gen, list(range(ngpu)))\n",
    "summary(gen,(3,64,64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nbQiATSjEf74"
   },
   "source": [
    "Defining the Discriminator network as given in the reference paper expect that a dropout in the last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DAQ63HYsEf75"
   },
   "outputs": [],
   "source": [
    "class Discriminator(n.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = n.Conv2d(3,64,3,padding=1,bias=False)\n",
    "        self.conv2 = n.Conv2d(64,64,3,stride=2,padding=1,bias=False)\n",
    "        self.bn2 = n.BatchNorm2d(64)\n",
    "        self.conv3 = n.Conv2d(64,128,3,padding=1,bias=False)\n",
    "        self.bn3 = n.BatchNorm2d(128)\n",
    "        self.conv4 = n.Conv2d(128,128,3,stride=2,padding=1,bias=False)\n",
    "        self.bn4 = n.BatchNorm2d(128)\n",
    "        self.conv5 = n.Conv2d(128,256,3,padding=1,bias=False)\n",
    "        self.bn5 = n.BatchNorm2d(256)\n",
    "        self.conv6 = n.Conv2d(256,256,3,stride=2,padding=1,bias=False)\n",
    "        self.bn6 = n.BatchNorm2d(256)\n",
    "        self.conv7 = n.Conv2d(256,512,3,padding=1,bias=False)\n",
    "        self.bn7 = n.BatchNorm2d(512)\n",
    "        self.conv8 = n.Conv2d(512,512,3,stride=2,padding=1,bias=False)\n",
    "        self.bn8 = n.BatchNorm2d(512)\n",
    "        self.fc1 = n.Linear(512*16*16,1024)\n",
    "        self.fc2 = n.Linear(1024,1)\n",
    "        self.drop = n.Dropout2d(0.3)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        block1 = f.leaky_relu(self.conv1(x))\n",
    "        block2 = f.leaky_relu(self.bn2(self.conv2(block1)))\n",
    "        block3 = f.leaky_relu(self.bn3(self.conv3(block2)))\n",
    "        block4 = f.leaky_relu(self.bn4(self.conv4(block3)))\n",
    "        block5 = f.leaky_relu(self.bn5(self.conv5(block4)))\n",
    "        block6 = f.leaky_relu(self.bn6(self.conv6(block5)))\n",
    "        block7 = f.leaky_relu(self.bn7(self.conv7(block6)))\n",
    "        block8 = f.leaky_relu(self.bn8(self.conv8(block7)))\n",
    "        block8 = block8.view(-1,block8.size(1)*block8.size(2)*block8.size(3))\n",
    "        block9 = f.leaky_relu(self.fc1(block8),)\n",
    "#         block9 = block9.view(-1,block9.size(1)*block9.size(2)*block9.size(3))\n",
    "        block10 = torch.sigmoid(self.drop(self.fc2(block9)))\n",
    "        return block9,block10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k2ub9ArZEf76"
   },
   "source": [
    "Assigning the discriminator to the gpu(if available) and printing the summary of the network with a dummy value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9GXi9cbgEf77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 256, 256]           1,728\n",
      "            Conv2d-2         [-1, 64, 128, 128]          36,864\n",
      "       BatchNorm2d-3         [-1, 64, 128, 128]             128\n",
      "            Conv2d-4        [-1, 128, 128, 128]          73,728\n",
      "       BatchNorm2d-5        [-1, 128, 128, 128]             256\n",
      "            Conv2d-6          [-1, 128, 64, 64]         147,456\n",
      "       BatchNorm2d-7          [-1, 128, 64, 64]             256\n",
      "            Conv2d-8          [-1, 256, 64, 64]         294,912\n",
      "       BatchNorm2d-9          [-1, 256, 64, 64]             512\n",
      "           Conv2d-10          [-1, 256, 32, 32]         589,824\n",
      "      BatchNorm2d-11          [-1, 256, 32, 32]             512\n",
      "           Conv2d-12          [-1, 512, 32, 32]       1,179,648\n",
      "      BatchNorm2d-13          [-1, 512, 32, 32]           1,024\n",
      "           Conv2d-14          [-1, 512, 16, 16]       2,359,296\n",
      "      BatchNorm2d-15          [-1, 512, 16, 16]           1,024\n",
      "           Linear-16                 [-1, 1024]     134,218,752\n",
      "           Linear-17                    [-1, 1]           1,025\n",
      "        Dropout2d-18                    [-1, 1]               0\n",
      "================================================================\n",
      "Total params: 138,906,945\n",
      "Trainable params: 138,906,945\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 118.01\n",
      "Params size (MB): 529.89\n",
      "Estimated Total Size (MB): 648.65\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nitin\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1331: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "disc = Discriminator().to(cuda)\n",
    "summary(disc,(3,256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_2JfIT4bEf79"
   },
   "outputs": [],
   "source": [
    "disc = Discriminator().to(cuda).float()\n",
    "gen = Generator().to(cuda).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kVO4XWIFEf7_"
   },
   "source": [
    "Downloading the pretrained vgg19 model from model module of torchvision library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KCbJX4oyEf7_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nitin\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\nitin\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to C:\\Users\\nitin/.cache\\torch\\hub\\checkpoints\\vgg19-dcbb9e9d.pth\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 548M/548M [03:51<00:00, 2.48MB/s]\n"
     ]
    }
   ],
   "source": [
    "vgg = models.vgg19(pretrained=True).to(cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MLIIT_rvEf8B"
   },
   "source": [
    "Defining the losses to be used in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rArq8bpwEf8B"
   },
   "outputs": [],
   "source": [
    "gen_loss = n.BCELoss()\n",
    "vgg_loss = n.MSELoss()\n",
    "mse_loss = n.MSELoss()\n",
    "disc_loss = n.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KpqcTlWKEf8D"
   },
   "source": [
    "Defining the adam optimizers for generator and discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dNyZ946bEf8D"
   },
   "outputs": [],
   "source": [
    "gen_optimizer = optim.Adam(gen.parameters(),lr=0.0001)\n",
    "disc_optimizer = optim.Adam(disc.parameters(),lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pN7aKN62Ef8G"
   },
   "source": [
    "Loading the images after based on resizing as numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GR_RwD4zEf8G"
   },
   "outputs": [],
   "source": [
    "def loadImages(imageList,path,resize=False):\n",
    "    images=[]\n",
    "    for image in (imageList):\n",
    "#         print(image)\n",
    "        if resize:\n",
    "            img = cv2.resize(cv2.imread(os.path.join(path,image)),(256,256)) \n",
    "        else:\n",
    "            img = cv2.imread(os.path.join(path,image))\n",
    "#         img = img.reshape(img.shape[2],img.shape[0],img.shape[1])\n",
    "#         print(img.shape)\n",
    "        img = np.moveaxis(img, 2, 0)\n",
    "#         print(img.shape)\n",
    "        images.append(img)\n",
    "    return np.array(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8PnAJcu1Ef8I"
   },
   "source": [
    "Converting the high resolution images into low resolution by applying gaussian blur, resizing in to 64*64 and loading it as numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-nv01VaFEf8I"
   },
   "outputs": [],
   "source": [
    "def loadLRImages(imagelist,path):\n",
    "    images=[]\n",
    "    for image in (imagelist):\n",
    "        img = cv2.resize(cv2.GaussianBlur(cv2.imread(os.path.join(path,image)),(5,5),cv2.BORDER_DEFAULT),(64,64)) \n",
    "#         img = img.reshape(img.shape[2],img.shape[0],img.shape[1])\n",
    "        img = np.moveaxis(img, 2, 0)\n",
    "        images.append(img)\n",
    "    return np.array(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xhpVOaMvEf8L"
   },
   "source": [
    "Loading the generator model from the given checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uoI7nAl8Ef8L"
   },
   "outputs": [],
   "source": [
    "def load_checkpoint(filepath):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model = checkpoint['model']\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    for parameter in model.parameters():\n",
    "        parameter.requires_grad = False\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fGQDWlS0Ef8N"
   },
   "source": [
    "Given low resolution images and checkpoint, Generating the high resolution images out of it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_MU7hpP6Ef8N"
   },
   "outputs": [],
   "source": [
    "def imagePostProcess(imagedir,modelPath):\n",
    "    imagelist=[]\n",
    "#     images = os.listdir(imagedir)\n",
    "    for img in imagedir:\n",
    "        img = cv2.resize(cv2.GaussianBlur(cv2.imread(os.path.join(hr_path,img)),(5,5),cv2.BORDER_DEFAULT),(64,64)) \n",
    "        imagelist.append(img)\n",
    "    imagearray = np.array(imagelist)/255\n",
    "#     imagearray = (imagedir)/255\n",
    "    # imagearrayPT = np.reshape(imagearray,(len(imagelist),imagearray.shape[3],imagearray.shape[1],imagearray.shape[2]))\n",
    "    imagearrayPT = np.moveaxis(imagearray,3,1)\n",
    "    # print(imagearrayPT.shape)\n",
    "\n",
    "    model = load_checkpoint(modelPath)\n",
    "    im_tensor = torch.from_numpy(imagearrayPT).float()\n",
    "    out_tensor = model(im_tensor)\n",
    "    # print(out_tensor.shape)\n",
    "    # out = np.reshape(out_tensor,[out_tensor.shape[0],out_tensor.shape[2],out_tensor.shape[3],out_tensor.shape[1]])\n",
    "    out = out_tensor.numpy()\n",
    "    out = np.moveaxis(out,1,3)\n",
    "    # print(out.shape)\n",
    "    out = np.clip(out,0,1)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LJ_ukw9GEf8P"
   },
   "source": [
    "Display utility of displaying images using matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aB-SFmTlEf8P"
   },
   "outputs": [],
   "source": [
    "def show_samples(sample_images):\n",
    "    figure, axes = plt.subplots(1, sample_images.shape[0], figsize = (10,10))\n",
    "    for index, axis in enumerate(axes):\n",
    "        axis.axis('off')\n",
    "        image_array = sample_images[index]\n",
    "        axis.imshow(image_array)\n",
    "        image = Image.fromarray((image_array * 255).astype('uint8'))\n",
    "    plt.savefig(os.path.join(base_path,\"out/SR\")+\"_\"+str(epoch)+\".png\", bbox_inches='tight', pad_inches=0)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xZBTT9lBEf8R"
   },
   "outputs": [],
   "source": [
    "#change the batch-size based on your system memory\n",
    "\n",
    "epochs=1000\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fzY7B-GCEf8T",
    "outputId": "50e0f9ae-7fce-4333-ac87-7fa8072c68cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "base_path = os.getcwd()\n",
    "\n",
    "#lr_path = os.path.join(base_path,\"trainImages\")\n",
    "hr_path =celebData\n",
    "#valid_path = os.path.join(base_path,\"SR_valid\")\n",
    "weight_file = os.path.join(base_path,\"SRPT_weights\")\n",
    "out_path = os.path.join(base_path,\"out\")\n",
    "\n",
    "if not os.path.exists(weight_file):\n",
    "    os.makedirs(weight_file)\n",
    "\n",
    "if not os.path.exists(out_path):\n",
    "    os.makedirs(out_path)\n",
    "\n",
    "    \n",
    "#LR_images_list = os.listdir(lr_path)\n",
    "HR_images_list = imageList\n",
    "batch_count = len(HR_images_list)//batch_size\n",
    "batch_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1pwc5pMhEf8U"
   },
   "source": [
    "Starting of training and defining losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 450
    },
    "colab_type": "code",
    "id": "L1p5wFAzEf8V",
    "outputId": "68224ed4-b26a-4d86-abad-881f7ab1e25f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|██████████▍                                                                     | 3/23 [14:22<1:38:33, 295.70s/it]"
     ]
    }
   ],
   "source": [
    "#batch_count=60\n",
    "for epoch in range(epochs):\n",
    "    d1loss_list=[]\n",
    "    d2loss_list=[]\n",
    "    gloss_list=[]\n",
    "    vloss_list=[]\n",
    "    mloss_list=[]\n",
    "    \n",
    "    for batch in tqdm(range(batch_count)):\n",
    "        hr_imagesList = [img for img in HR_images_list[batch*batch_size:(batch+1)*batch_size]]\n",
    "        lr_images = loadLRImages(hr_imagesList,hr_path)/255\n",
    "        hr_images = loadImages(hr_imagesList,hr_path,True)/255\n",
    "        \n",
    "                \n",
    "        disc.zero_grad()\n",
    "\n",
    "        gen_out = gen(torch.from_numpy(lr_images).to(cuda).float())\n",
    "        _,f_label = disc(gen_out)\n",
    "        _,r_label = disc(torch.from_numpy(hr_images).to(cuda).float())\n",
    "        d1_loss = (disc_loss(f_label,torch.zeros_like(f_label,dtype=torch.float)))\n",
    "        d2_loss = (disc_loss(r_label,torch.ones_like(r_label,dtype=torch.float)))\n",
    "        # d_loss = d1_loss+d2_loss\n",
    "        d2_loss.backward()\n",
    "        d1_loss.backward(retain_graph=True)\n",
    "        # print(d1_loss,d2_loss)\n",
    "#         d_loss.backward(retain_graph=True)\n",
    "        disc_optimizer.step()\n",
    "        \n",
    "\n",
    "        gen.zero_grad()      \n",
    "        g_loss = gen_loss(f_label.data,torch.ones_like(f_label,dtype=torch.float))\n",
    "        v_loss = vgg_loss(vgg.features[:7](gen_out),vgg.features[:7](torch.from_numpy(hr_images).to(cuda).float()))\n",
    "        m_loss = mse_loss(gen_out,torch.from_numpy(hr_images).to(cuda).float())\n",
    "        \n",
    "        generator_loss = g_loss + v_loss + m_loss\n",
    "        # v_loss.backward(retain_graph=True)\n",
    "        # m_loss.backward(retain_graph=True)\n",
    "        # g_loss.backward()\n",
    "        # print(generator_loss)\n",
    "\n",
    "        generator_loss.backward()\n",
    "        gen_optimizer.step()\n",
    "        \n",
    "        d1loss_list.append(d1_loss.item())\n",
    "        d2loss_list.append(d2_loss.item())\n",
    "        \n",
    "        gloss_list.append(g_loss.item())\n",
    "        vloss_list.append(v_loss.item())\n",
    "        mloss_list.append(m_loss.item())\n",
    "\n",
    "        \n",
    "        \n",
    "#         print(\"d1Loss ::: \"+str((d1_loss.item()))+\" d2Loss ::: \"+str((d2_loss.item())))\n",
    "#         print(\"gloss ::: \"+str((g_loss.item()))+\" vloss ::: \"+str((v_loss.item()))+\" mloss ::: \"+str((m_loss.item())))\n",
    "    print(\"Epoch ::::  \"+str(epoch+1)+\"  d1_loss ::: \"+str(np.mean(d1loss_list))+\"  d2_loss :::\"+str(np.mean(d2loss_list)))\n",
    "    print(\"genLoss ::: \"+str(np.mean(gloss_list))+\"  vggLoss ::: \"+str(np.mean(vloss_list))+\"  MeanLoss  ::: \"+str(np.mean(mloss_list)))\n",
    "    \n",
    "    if(epoch%3==0):\n",
    "        \n",
    "        checkpoint = {'model': Generator(),\n",
    "              'input_size': 64,\n",
    "              'output_size': 256,\n",
    "              'state_dict': gen.state_dict()}\n",
    "        torch.save(checkpoint,os.path.join(weight_file,\"SR\"+str(epoch+1)+\".pth\"))\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        out_images = imagePostProcess(images[-2:],os.path.join(weight_file,\"SR\"+str(epoch+1)+\".pth\"))\n",
    "#         print(out_images.shape)\n",
    "#         test_images = loadLRImages(images[:-3],hr_path)/255\n",
    "#         test_images = np.reshape(test_images,(test_images[0],test_images.shape[3],test_images.shape[1],test_images.shape[2]))\n",
    "#         out_images = gen(torch.from_numpy(test_images).to(cuda).float())\n",
    "#         out_images = np.reshape(out_images,(out_images[0],out_images[2],out_images[3],out_images[1]))\n",
    "        show_samples(out_images)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "SR_PT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
